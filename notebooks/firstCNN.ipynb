{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv \n",
    "from torchvision import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.read_image does not support B&W PNG images\n",
    "def load_sat_image(file):\n",
    "    return io.read_image(file).to(dtype=torch.float32)#.to(device, dtype=torch.float32)\n",
    "\n",
    "def load_groundtruth_image(file):\n",
    "    return torch.as_tensor(mpimg.imread(file), dtype = torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(20, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_sat_image(image_dir + files[i]) for i in range(n)]\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_groundtruth_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "n = 10 # Only use 10 images for training\n",
    "\n",
    "training_images = list(zip(imgs, gt_imgs))[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(sat_img, gt_img, size) :\n",
    "    pad = int(size / 2)\n",
    "    padded = F.pad(sat_img, (pad, pad, pad, pad))\n",
    "    res = []\n",
    "    for i in range(gt_img.size(0)):\n",
    "        for j in range(gt_img.size(1)):\n",
    "            res.append((padded[:, i : i + size , j : j + size], gt_img[i, j]))\n",
    "    return res\n",
    "\n",
    "training_data = []\n",
    "for sat_img, gt_img in training_images:\n",
    "    training_data.extend(crop_image(sat_img, gt_img, 31))\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_data[0][0].permute(1, 2, 0).to(dtype=torch.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        Model that takes a 3 colors 31 x 31 images and \n",
    "        returns a single value which correspond to the \n",
    "        probability of the center pixel being a road (1)\n",
    "        or not (0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Input 3 colors 31 x 31 images\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=10,\n",
    "            kernel_size=15,\n",
    "            padding=7 # (kernel_size - 1) / 2\n",
    "        )\n",
    "        # 10 channels 31 x 31\n",
    "        self.pool1 = nn.AvgPool2d(\n",
    "            kernel_size=5,\n",
    "            padding=2\n",
    "        )\n",
    "        # 10 channels 7 x 7 (14 = (31 + 2 * padding - kernel_size) / kernel_size + 1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=10, \n",
    "            out_channels=20,\n",
    "            kernel_size=4,\n",
    "        )\n",
    "        # 20 channels 4 x 4 (4 = 7 - kernel_size + 1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(20 * 4 * 4, 100)\n",
    "        self.lin2 = nn.Linear(100, 10)\n",
    "        self.lin3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        \n",
    "        x = torch.sigmoid(self.lin1(x))\n",
    "        x = torch.sigmoid(self.lin2(x))\n",
    "        x = torch.sigmoid(self.lin3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Model().to(device=device)\n",
    "print(model)\n",
    "print(sum([np.prod(p.size()) for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([training_data[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(training_data) // 10) :\n",
    "    optimiser.zero_grad()\n",
    "    input = torch.stack([x for x, _ in training_data[i : i + 10]])\n",
    "    output = model(input)\n",
    "    loss = criterion(output, torch.stack([y for _, y in training_data[i : i + 10]]).view(10, 1))\n",
    "    if i % 1600 == 0 :\n",
    "        print(f\"epoch {i} ({i/1600}%), loss = {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sat_image(\"data/training/images/satImage_001.png\")[0][0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_background_image(\"data/training/groundtruth/satImage_001.png\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "images = tv.datasets.ImageFolder(root=root_dir, transform=tv.transforms.ToTensor())\n",
    "files = os.listdir(image_dir)\n",
    "n = min(20, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [io.read_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [io.image.read_file(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "n = 10 # Only use 10 images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_imgs[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Image size = ' + str(imgs[0].shape[0]) + ',' + str(imgs[0].shape[1]))\n",
    "\n",
    "# Show first image and its groundtruth image\n",
    "cimg = concatenate_images(imgs[0], gt_imgs[0])\n",
    "fig1 = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cimg, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "\n",
    "tv.io.read_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute features for each image patch\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def value_to_class(v):\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X = np.asarray([ extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print feature statistics\n",
    "\n",
    "print('Computed ' + str(X.shape[0]) + ' features')\n",
    "print('Feature dimension = ' + str(X.shape[1]))\n",
    "print('Number of classes = ' + str(np.max(Y)))  #TODO: fix, length(unique(Y)) \n",
    "\n",
    "Y0 = [i for i, j in enumerate(Y) if j == 0]\n",
    "Y1 = [i for i, j in enumerate(Y) if j == 1]\n",
    "print('Class 0: ' + str(len(Y0)) + ' samples')\n",
    "print('Class 1: ' + str(len(Y1)) + ' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display a patch that belongs to the foreground class\n",
    "plt.imshow(gt_patches[Y1[3]], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot 2d features using groundtruth to color the datapoints\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        Model that takes a 3 colors 31 x 31 images and \n",
    "        returns a single value which correspond to the \n",
    "        probability of the center pixel being a road (1)\n",
    "        or not (0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Input 3 colors 31 x 31 images\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=10,\n",
    "            kernel_size=15,\n",
    "            padding=7 # (kernel_size - 1) / 2\n",
    "        )\n",
    "        # 10 channels 31 x 31\n",
    "        self.pool1 = nn.AvgPool2d(\n",
    "            kernel_size=5,\n",
    "            padding=2\n",
    "        )\n",
    "        # 10 channels 7 x 7 (14 = (31 + 2 * padding - kernel_size) / kernel_size + 1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=10, \n",
    "            out_channels=20,\n",
    "            kernel_size=4,\n",
    "        )\n",
    "        # 20 channels 4 x 4 (4 = 7 - kernel_size + 1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(20 * 4 * 4, 100)\n",
    "        self.lin2 = nn.Linear(100, 10)\n",
    "        self.lin3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        print(x.size())\n",
    "        x = self.pool1(x)\n",
    "        print(x.size())\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        print(x.size())\n",
    "\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        \n",
    "        x = torch.sigmoid(self.lin1(x))\n",
    "        x = torch.sigmoid(self.lin2(x))\n",
    "        x = torch.sigmoid(self.lin3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Model().to(device=device)\n",
    "print(model)\n",
    "print(sum([np.prod(p.size()) for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 31, 31))\n",
    "out = model(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "Z = model(X)\n",
    "\n",
    "# Get non-zeros in prediction and grountruth arrays\n",
    "Zn = np.nonzero(Z)[0]\n",
    "Yn = np.nonzero(Y)[0]\n",
    "\n",
    "TPR = len(list(set(Yn) & set(Zn))) / float(len(Z))\n",
    "print('True positive rate = ' + str(TPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot features using predictions to color datapoints\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Z, edgecolors='k', cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert array of labels to an image\n",
    "\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    im = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            im[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return im\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*255\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run prediction on the img_idx-th image\n",
    "img_idx = 12\n",
    "\n",
    "Xi = extract_img_features(image_dir + files[img_idx])\n",
    "Zi = logreg.predict(Xi)\n",
    "plt.scatter(Xi[:, 0], Xi[:, 1], c=Zi, edgecolors='k', cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display prediction as an image\n",
    "\n",
    "w = gt_imgs[img_idx].shape[0]\n",
    "h = gt_imgs[img_idx].shape[1]\n",
    "predicted_im = label_to_img(w, h, patch_size, patch_size, Zi)\n",
    "cimg = concatenate_images(imgs[img_idx], predicted_im)\n",
    "fig1 = plt.figure(figsize=(10, 10)) # create a figure with the default size \n",
    "plt.imshow(cimg, cmap='Greys_r')\n",
    "\n",
    "new_img = make_img_overlay(imgs[img_idx], predicted_im)\n",
    "\n",
    "plt.imshow(new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
